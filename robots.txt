# robots.txt for Mushood's Portfolio
# https://mushood123.github.io/

# Allow all search engines to crawl the entire site
User-agent: *
Allow: /

# Sitemap location (using sitemap index for better organization)
Sitemap: https://mushood123.github.io/sitemap-index.xml

# Crawl-delay (optional, helps prevent server overload)
# Most modern search engines ignore this, but it's good practice
Crawl-delay: 1

# Specific rules for major search engines
# Google
User-agent: Googlebot
Allow: /
Crawl-delay: 0

# Google Images
User-agent: Googlebot-Image
Allow: /

# Bing
User-agent: Bingbot
Allow: /
Crawl-delay: 0

# Yahoo
User-agent: Slurp
Allow: /

# DuckDuckGo
User-agent: DuckDuckBot
Allow: /

# Yandex
User-agent: Yandex
Allow: /

# Baidu
User-agent: Baiduspider
Allow: /

# Block bad bots and scrapers (optional security measure)
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Allow access to important resources
Allow: /assets/css/
Allow: /assets/js/
Allow: /assets/images/
Allow: /assets/resume/

# Disallow access to sensitive or unnecessary files (if any exist)
Disallow: /node_modules/
Disallow: /.git/
Disallow: /.vscode/
Disallow: /.idea/
Disallow: /package.json
Disallow: /package-lock.json
Disallow: /.gitignore
Disallow: /.env

# Allow social media crawlers
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: WhatsApp
Allow: /
